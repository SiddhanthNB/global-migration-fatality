{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b10a5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import randint\n",
    "from category_encoders import TargetEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('data/Cleaned_Migrant_Fatalities_Data.csv')\n",
    "\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_rename_map = {\n",
    "    'Region of Incident': 'region_incident',\n",
    "    'Incident Date': 'incident_date',\n",
    "    'Incident Year': 'incident_year',\n",
    "    'Month': 'month_name', # Temporarily keep for review, then drop\n",
    "    'Number of Dead': 'num_dead',\n",
    "    'Minimum Estimated Number of Missing': 'min_est_missing',\n",
    "    'Total Number of Dead and Missing': 'total_dead_missing',\n",
    "    'Number of Survivors': 'num_survivors',\n",
    "    'Number of Females': 'num_females',\n",
    "    'Number of Males': 'num_males',\n",
    "    'Number of Children': 'num_children',\n",
    "    'Country of Origin': 'country_origin',\n",
    "    'Region of Origin': 'region_origin',\n",
    "    'Cause of Death': 'cause_death',\n",
    "    'Country of Incident': 'country_incident',\n",
    "    'Migration Route': 'migration_route',\n",
    "    'Coordinates': 'coordinates', # Temporarily keep for review, then drop\n",
    "    'UNSD Geographical Grouping': 'unsd_geo_grouping', # Temporarily keep for review, then drop\n",
    "    'Year': 'year_duplicate', # Temporarily keep for review, then drop\n",
    "    'Month_Num': 'incident_month' # Renaming to incident_month as confirmed\n",
    "}\n",
    "df_raw = df_raw.rename(columns=column_rename_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cols = [\n",
    "    'region_incident', 'country_incident', 'incident_date', 'incident_year', 'incident_month',\n",
    "    'region_origin', 'country_origin', 'cause_death', 'migration_route',\n",
    "    'min_est_missing', 'total_dead_missing', 'num_survivors','num_females', 'num_males', 'num_children', 'num_dead'\n",
    "]\n",
    "\n",
    "df_raw = df_raw[filtered_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28760302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['incident_date'] = pd.to_datetime(df_raw['incident_date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rows_before_date_drop = df_raw.shape[0]\n",
    "df_raw.dropna(subset=['incident_date'], inplace=True)\n",
    "print(f\"\\nDropped {original_rows_before_date_drop - df_raw.shape[0]} rows due to null 'incident_date'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['incident_year'] = df_raw['incident_year'].astype('Int64')\n",
    "df_raw['incident_month'] = df_raw['incident_month'].astype('Int64')\n",
    "df_raw['incident_week'] = df_raw['incident_date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0e6c43b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape after weekly aggregation: (4293, 16)\n",
      "\n",
      "--- Columns after weekly aggregation ---\n",
      "['region_incident', 'incident_year', 'incident_week', 'total_dead_missing', 'num_dead', 'min_est_missing', 'num_survivors', 'num_females', 'num_males', 'num_children', 'num_incidents', 'most_freq_country_origin', 'most_freq_region_origin', 'most_freq_cause_death', 'most_freq_country_incident', 'most_freq_migration_route']\n",
      "\n",
      "Original number of rows (after incident_date drop): 19212\n",
      "Number of rows after weekly aggregation (weeks with incidents): 4293\n"
     ]
    }
   ],
   "source": [
    "df_weekly_aggregated = df_raw.groupby(['region_incident', 'incident_year', 'incident_week']).agg(\n",
    "    # Sum of numerical columns\n",
    "    total_dead_missing=('total_dead_missing', 'sum'),\n",
    "    num_dead=('num_dead', 'sum'),\n",
    "    min_est_missing=('min_est_missing', 'sum'),\n",
    "    num_survivors=('num_survivors', 'sum'),\n",
    "    num_females=('num_females', 'sum'),\n",
    "    num_males=('num_males', 'sum'),\n",
    "    num_children=('num_children', 'sum'),\n",
    "    # Count of original incidents in each aggregated unit\n",
    "    num_incidents=('incident_date', 'count'),\n",
    "\n",
    "    # Aggregating categorical features: taking the most frequent value (mode)\n",
    "    # dropna=False ensures NaN values are treated as a distinct category if they are the most frequent.\n",
    "    # Handling cases where x.mode() might be empty (e.g., all NaNs in a group)\n",
    "    most_freq_country_origin=('country_origin', lambda x: x.mode(dropna=False)[0] if not x.mode(dropna=False).empty else 'Unknown'),\n",
    "    most_freq_region_origin=('region_origin', lambda x: x.mode(dropna=False)[0] if not x.mode(dropna=False).empty else 'Unknown'),\n",
    "    most_freq_cause_death=('cause_death', lambda x: x.mode(dropna=False)[0] if not x.mode(dropna=False).empty else 'Unknown'),\n",
    "    most_freq_country_incident=('country_incident', lambda x: x.mode(dropna=False)[0] if not x.mode(dropna=False).empty else 'Unknown'),\n",
    "    most_freq_migration_route=('migration_route', lambda x: x.mode(dropna=False)[0] if not x.mode(dropna=False).empty else 'Unknown'),\n",
    "\n",
    "    # # Aggregating categorical features: count of unique values in each aggregated unit\n",
    "    # num_unique_country_origin=('country_origin', 'nunique'),\n",
    "    # num_unique_region_origin=('region_origin', 'nunique'),\n",
    "    # num_unique_cause_death=('cause_death', 'nunique'),\n",
    "    # num_unique_country_incident=('country_incident', 'nunique'),\n",
    "    # num_unique_migration_route=('migration_route', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "df_weekly_aggregated['incident_year'] = df_weekly_aggregated['incident_year'].astype(int)\n",
    "df_weekly_aggregated['incident_week'] = df_weekly_aggregated['incident_week'].astype(int)\n",
    "\n",
    "# print shape after aggregation\n",
    "print(f\"\\nShape after weekly aggregation: {df_weekly_aggregated.shape}\")\n",
    "\n",
    "# print columns after aggregation\n",
    "print(\"\\n--- Columns after weekly aggregation ---\")\n",
    "print(df_weekly_aggregated.columns.tolist())\n",
    "\n",
    "print(f\"\\nOriginal number of rows (after incident_date drop): {df_raw.shape[0]}\")\n",
    "print(f\"Number of rows after weekly aggregation (weeks with incidents): {df_weekly_aggregated.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4aff10fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape after filling NaNs: (7049, 16)\n",
      "\n",
      "--- df_expanded_weekly Info after filling NaNs ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7049 entries, 0 to 7048\n",
      "Data columns (total 16 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   region_incident             7049 non-null   object \n",
      " 1   incident_year               7049 non-null   int64  \n",
      " 2   incident_week               7049 non-null   int64  \n",
      " 3   total_dead_missing          7049 non-null   float64\n",
      " 4   num_dead                    7049 non-null   float64\n",
      " 5   min_est_missing             7049 non-null   float64\n",
      " 6   num_survivors               7049 non-null   float64\n",
      " 7   num_females                 7049 non-null   float64\n",
      " 8   num_males                   7049 non-null   float64\n",
      " 9   num_children                7049 non-null   float64\n",
      " 10  num_incidents               7049 non-null   float64\n",
      " 11  most_freq_country_origin    7049 non-null   object \n",
      " 12  most_freq_region_origin     7049 non-null   object \n",
      " 13  most_freq_cause_death       7049 non-null   object \n",
      " 14  most_freq_country_incident  7049 non-null   object \n",
      " 15  most_freq_migration_route   7049 non-null   object \n",
      "dtypes: float64(8), int64(2), object(6)\n",
      "memory usage: 881.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "unique_regions = df_weekly_aggregated['region_incident'].unique()\n",
    "\n",
    "min_year = df_weekly_aggregated['incident_year'].min()\n",
    "max_year = df_weekly_aggregated['incident_year'].max()\n",
    "\n",
    "all_weekly_dates = []\n",
    "for year in range(min_year, max_year + 1):\n",
    "    # Determine the maximum week number for the given year (can be 52 or 53)\n",
    "    max_week_in_year = pd.to_datetime(f'{year}-12-31').isocalendar().week\n",
    "    for week in range(1, max_week_in_year + 1):\n",
    "        all_weekly_dates.append({'incident_year': year, 'incident_week': week})\n",
    "full_weekly_date_range_df = pd.DataFrame(all_weekly_dates)\n",
    "\n",
    "full_weekly_time_series_df = pd.merge(\n",
    "    pd.DataFrame({'region_incident': unique_regions}),\n",
    "    full_weekly_date_range_df,\n",
    "    how='cross'\n",
    ")\n",
    "\n",
    "full_weekly_time_series_df['incident_year'] = full_weekly_time_series_df['incident_year'].astype(int)\n",
    "full_weekly_time_series_df['incident_week'] = full_weekly_time_series_df['incident_week'].astype(int)\n",
    "\n",
    "df_expanded_weekly = pd.merge(\n",
    "    full_weekly_time_series_df,\n",
    "    df_weekly_aggregated,\n",
    "    on=['region_incident', 'incident_year', 'incident_week'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "categorical_cols_to_fill_no_incidents = [\n",
    "    'most_freq_country_origin', 'most_freq_region_origin', 'most_freq_cause_death',\n",
    "    'most_freq_country_incident', 'most_freq_migration_route'\n",
    "]\n",
    "\n",
    "numerical_cols_to_fill_zero = [\n",
    "    'total_dead_missing', 'num_dead', 'min_est_missing',\n",
    "    'num_survivors', 'num_females', 'num_males',\n",
    "    'num_children', 'num_incidents',\n",
    "    # 'num_unique_country_origin', 'num_unique_region_origin',\n",
    "    # 'num_unique_cause_death', 'num_unique_country_incident',\n",
    "    # 'num_unique_migration_route'\n",
    "]\n",
    "\n",
    "# Fill numerical columns with 0\n",
    "for col in numerical_cols_to_fill_zero:\n",
    "    if col in df_expanded_weekly.columns:\n",
    "        df_expanded_weekly[col] = df_expanded_weekly[col].fillna(0)\n",
    "\n",
    "# Fill categorical columns with 'No Incidents'\n",
    "for col in categorical_cols_to_fill_no_incidents:\n",
    "    if col in df_expanded_weekly.columns:\n",
    "        df_expanded_weekly[col] = df_expanded_weekly[col].fillna('No Incidents')\n",
    "\n",
    "# Sort the expanded DataFrame for correct lag/rolling calculations later\n",
    "df_expanded_weekly = df_expanded_weekly.sort_values(\n",
    "    by=['region_incident', 'incident_year', 'incident_week']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nShape after filling NaNs: {df_expanded_weekly.shape}\")\n",
    "\n",
    "print(\"\\n--- df_expanded_weekly Info after filling NaNs ---\")\n",
    "print(df_expanded_weekly.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "07f7d8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalculated 75th percentile threshold for 'total_dead_missing' (weekly expanded data) is: 3.00\n",
      "\n",
      "--- Distribution of 'is_high_risk' target variable (Weekly) ---\n",
      "is_high_risk\n",
      "0    5062\n",
      "1    1987\n",
      "\n",
      "--- Percentage distribution of 'is_high_risk' target variable (Weekly) ---\n",
      "is_high_risk\n",
      "0    0.718116\n",
      "1    0.281884\n"
     ]
    }
   ],
   "source": [
    "# Recalculate the 75th percentile threshold on the expanded weekly data\n",
    "weekly_high_fatality_threshold = df_expanded_weekly['total_dead_missing'].quantile(0.75)\n",
    "print(f\"Recalculated 75th percentile threshold for 'total_dead_missing' (weekly expanded data) is: {weekly_high_fatality_threshold:.2f}\")\n",
    "\n",
    "# Create the 'is_high_risk' target variable\n",
    "df_expanded_weekly['is_high_risk'] = (df_expanded_weekly['total_dead_missing'] >= weekly_high_fatality_threshold).astype(int)\n",
    "\n",
    "# Check the distribution of the new target variable\n",
    "print(\"\\n--- Distribution of 'is_high_risk' target variable (Weekly) ---\")\n",
    "print(df_expanded_weekly['is_high_risk'].value_counts().to_string())\n",
    "\n",
    "print(\"\\n--- Percentage distribution of 'is_high_risk' target variable (Weekly) ---\")\n",
    "print(df_expanded_weekly['is_high_risk'].value_counts(normalize=True).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "706989e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Created 15 lag/rolling features ---\n",
      "Features created: ['num_survivors_lag1', 'num_survivors_lag2', 'num_survivors_lag3', 'num_incidents_lag1', 'num_incidents_lag2', 'num_incidents_lag3', 'num_females_lag1', 'num_females_lag2', 'num_females_lag3', 'num_males_lag1', 'num_males_lag2', 'num_males_lag3', 'num_children_lag1', 'num_children_lag2', 'num_children_lag3']\n",
      "\n",
      "--- df_expanded_weekly Info after Reduced Lag and Rolling Features ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7049 entries, 0 to 7048\n",
      "Data columns (total 32 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   region_incident             7049 non-null   object \n",
      " 1   incident_year               7049 non-null   int64  \n",
      " 2   incident_week               7049 non-null   int64  \n",
      " 3   total_dead_missing          7049 non-null   float64\n",
      " 4   num_dead                    7049 non-null   float64\n",
      " 5   min_est_missing             7049 non-null   float64\n",
      " 6   num_survivors               7049 non-null   float64\n",
      " 7   num_females                 7049 non-null   float64\n",
      " 8   num_males                   7049 non-null   float64\n",
      " 9   num_children                7049 non-null   float64\n",
      " 10  num_incidents               7049 non-null   float64\n",
      " 11  most_freq_country_origin    7049 non-null   object \n",
      " 12  most_freq_region_origin     7049 non-null   object \n",
      " 13  most_freq_cause_death       7049 non-null   object \n",
      " 14  most_freq_country_incident  7049 non-null   object \n",
      " 15  most_freq_migration_route   7049 non-null   object \n",
      " 16  is_high_risk                7049 non-null   int64  \n",
      " 17  num_survivors_lag1          7049 non-null   float64\n",
      " 18  num_survivors_lag2          7049 non-null   float64\n",
      " 19  num_survivors_lag3          7049 non-null   float64\n",
      " 20  num_incidents_lag1          7049 non-null   float64\n",
      " 21  num_incidents_lag2          7049 non-null   float64\n",
      " 22  num_incidents_lag3          7049 non-null   float64\n",
      " 23  num_females_lag1            7049 non-null   float64\n",
      " 24  num_females_lag2            7049 non-null   float64\n",
      " 25  num_females_lag3            7049 non-null   float64\n",
      " 26  num_males_lag1              7049 non-null   float64\n",
      " 27  num_males_lag2              7049 non-null   float64\n",
      " 28  num_males_lag3              7049 non-null   float64\n",
      " 29  num_children_lag1           7049 non-null   float64\n",
      " 30  num_children_lag2           7049 non-null   float64\n",
      " 31  num_children_lag3           7049 non-null   float64\n",
      "dtypes: float64(23), int64(3), object(6)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "\n",
      "--- First 5 rows of after adding temporal features ---\n",
      "region_incident  incident_year  incident_week  total_dead_missing  num_dead  min_est_missing  num_survivors  num_females  num_males  num_children  num_incidents most_freq_country_origin most_freq_region_origin most_freq_cause_death most_freq_country_incident most_freq_migration_route  is_high_risk  num_survivors_lag1  num_survivors_lag2  num_survivors_lag3  num_incidents_lag1  num_incidents_lag2  num_incidents_lag3  num_females_lag1  num_females_lag2  num_females_lag3  num_males_lag1  num_males_lag2  num_males_lag3  num_children_lag1  num_children_lag2  num_children_lag3\n",
      "      Caribbean           2014              1                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents              No Incidents             0                 0.0                 0.0                 0.0                 0.0                 0.0                 0.0               0.0               0.0               0.0             0.0             0.0             0.0                0.0                0.0                0.0\n",
      "      Caribbean           2015              1                 1.0       1.0              0.0            3.0          0.0        1.0           0.0            1.0                     Cuba               Caribbean              Drowning             Cayman Islands              No Incidents             0                 0.0                 0.0                 0.0                 0.0                 0.0                 0.0               0.0               0.0               0.0             0.0             0.0             0.0                0.0                0.0                0.0\n",
      "      Caribbean           2015              2                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents              No Incidents             0                 3.0                 0.0                 0.0                 1.0                 0.0                 0.0               0.0               0.0               0.0             1.0             0.0             0.0                0.0                0.0                0.0\n",
      "      Caribbean           2015              3                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents              No Incidents             0                 0.0                 3.0                 0.0                 0.0                 1.0                 0.0               0.0               0.0               0.0             0.0             1.0             0.0                0.0                0.0                0.0\n",
      "      Caribbean           2015              4                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents              No Incidents             0                 0.0                 0.0                 3.0                 0.0                 0.0                 1.0               0.0               0.0               0.0             0.0             0.0             1.0                0.0                0.0                0.0\n"
     ]
    }
   ],
   "source": [
    "# Numerical columns for which to create lag and rolling features\n",
    "# Focus only on migration activity patterns, not fatality outcomes\n",
    "numerical_cols_for_lags_and_rolls = [\n",
    "    'num_survivors',     # Rescue/safety patterns (not direct fatality measure)\n",
    "    'num_incidents',     # Migration activity level\n",
    "    'num_females',       # Demographics patterns\n",
    "    'num_males',         # Demographics patterns\n",
    "    'num_children'       # Vulnerable population patterns\n",
    "]\n",
    "\n",
    "# Loop through selected numerical columns to create lag and rolling features\n",
    "for col in numerical_cols_for_lags_and_rolls:\n",
    "    # Create Lag Features (1-week lag)\n",
    "    df_expanded_weekly[f'{col}_lag1'] = df_expanded_weekly.groupby('region_incident')[col].shift(1)\n",
    "    df_expanded_weekly[f'{col}_lag2'] = df_expanded_weekly.groupby('region_incident')[col].shift(2)\n",
    "    df_expanded_weekly[f'{col}_lag3'] = df_expanded_weekly.groupby('region_incident')[col].shift(3)\n",
    "\n",
    "    # Create Rolling Window Features (4-week average for monthly trends)\n",
    "    # df_expanded_weekly[f'{col}_rolling4w_emwa'] = df_expanded_weekly.groupby('region_incident')[col].transform(\n",
    "    #     lambda x: x.ewm(span=4, min_periods=1).mean()\n",
    "    # )\n",
    "\n",
    "# # Create lag/rolling features for route diversity (indicates migration pressure)\n",
    "# df_expanded_weekly['num_unique_migration_route_lag1'] = df_expanded_weekly.groupby('region_incident')['num_unique_migration_route'].shift(1)\n",
    "# df_expanded_weekly['num_unique_migration_route_rolling4w_emwa'] = df_expanded_weekly.groupby('region_incident')['num_unique_migration_route'].transform(\n",
    "#     lambda x: x.rolling(window=4, min_periods=1).mean()\n",
    "# )\n",
    "\n",
    "# Fill NaNs created by lags/rolling windows\n",
    "lag_rolling_cols = [col for col in df_expanded_weekly.columns if '_lag' in col or '_rolling4w_emwa' in col]\n",
    "for col in lag_rolling_cols:\n",
    "    df_expanded_weekly[col] = df_expanded_weekly[col].fillna(0)\n",
    "\n",
    "print(f\"\\n--- Created {len(lag_rolling_cols)} lag/rolling features ---\")\n",
    "print(f\"Features created: {lag_rolling_cols}\")\n",
    "\n",
    "print(\"\\n--- df_expanded_weekly Info after Reduced Lag and Rolling Features ---\")\n",
    "print(df_expanded_weekly.info())\n",
    "\n",
    "print(\"\\n--- First 5 rows of after adding temporal features ---\")\n",
    "print(df_expanded_weekly.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b70318b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- df_expanded_weekly Info after adding pct_change and cyclical week features ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7049 entries, 0 to 7048\n",
      "Data columns (total 34 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   region_incident             7049 non-null   object \n",
      " 1   incident_year               7049 non-null   int64  \n",
      " 2   incident_week               7049 non-null   int64  \n",
      " 3   total_dead_missing          7049 non-null   float64\n",
      " 4   num_dead                    7049 non-null   float64\n",
      " 5   min_est_missing             7049 non-null   float64\n",
      " 6   num_survivors               7049 non-null   float64\n",
      " 7   num_females                 7049 non-null   float64\n",
      " 8   num_males                   7049 non-null   float64\n",
      " 9   num_children                7049 non-null   float64\n",
      " 10  num_incidents               7049 non-null   float64\n",
      " 11  most_freq_country_origin    7049 non-null   object \n",
      " 12  most_freq_region_origin     7049 non-null   object \n",
      " 13  most_freq_cause_death       7049 non-null   object \n",
      " 14  most_freq_country_incident  7049 non-null   object \n",
      " 15  most_freq_migration_route   7049 non-null   object \n",
      " 16  is_high_risk                7049 non-null   int64  \n",
      " 17  num_survivors_lag1          7049 non-null   float64\n",
      " 18  num_survivors_lag2          7049 non-null   float64\n",
      " 19  num_survivors_lag3          7049 non-null   float64\n",
      " 20  num_incidents_lag1          7049 non-null   float64\n",
      " 21  num_incidents_lag2          7049 non-null   float64\n",
      " 22  num_incidents_lag3          7049 non-null   float64\n",
      " 23  num_females_lag1            7049 non-null   float64\n",
      " 24  num_females_lag2            7049 non-null   float64\n",
      " 25  num_females_lag3            7049 non-null   float64\n",
      " 26  num_males_lag1              7049 non-null   float64\n",
      " 27  num_males_lag2              7049 non-null   float64\n",
      " 28  num_males_lag3              7049 non-null   float64\n",
      " 29  num_children_lag1           7049 non-null   float64\n",
      " 30  num_children_lag2           7049 non-null   float64\n",
      " 31  num_children_lag3           7049 non-null   float64\n",
      " 32  incident_week_sin           7049 non-null   float64\n",
      " 33  incident_week_cos           7049 non-null   float64\n",
      "dtypes: float64(25), int64(3), object(6)\n",
      "memory usage: 1.8+ MB\n",
      "None\n",
      "\n",
      "--- First 10 rows of df_expanded_weekly with new temporal features ---\n",
      "region_incident  incident_year  incident_week  total_dead_missing  num_dead  min_est_missing  num_survivors  num_females  num_males  num_children  num_incidents most_freq_country_origin most_freq_region_origin most_freq_cause_death most_freq_country_incident   most_freq_migration_route  is_high_risk  num_survivors_lag1  num_survivors_lag2  num_survivors_lag3  num_incidents_lag1  num_incidents_lag2  num_incidents_lag3  num_females_lag1  num_females_lag2  num_females_lag3  num_males_lag1  num_males_lag2  num_males_lag3  num_children_lag1  num_children_lag2  num_children_lag3  incident_week_sin  incident_week_cos\n",
      "      Caribbean           2014              1                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents                No Incidents             0                 0.0                 0.0                 0.0                 0.0                 0.0                 0.0               0.0               0.0               0.0             0.0             0.0             0.0                0.0                0.0                0.0           0.118273           0.992981\n",
      "      Caribbean           2015              1                 1.0       1.0              0.0            3.0          0.0        1.0           0.0            1.0                     Cuba               Caribbean              Drowning             Cayman Islands                No Incidents             0                 0.0                 0.0                 0.0                 0.0                 0.0                 0.0               0.0               0.0               0.0             0.0             0.0             0.0                0.0                0.0                0.0           0.118273           0.992981\n",
      "      Caribbean           2015              2                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents                No Incidents             0                 3.0                 0.0                 0.0                 1.0                 0.0                 0.0               0.0               0.0               0.0             1.0             0.0             0.0                0.0                0.0                0.0           0.234886           0.972023\n",
      "      Caribbean           2015              3                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents                No Incidents             0                 0.0                 3.0                 0.0                 0.0                 1.0                 0.0               0.0               0.0               0.0             0.0             1.0             0.0                0.0                0.0                0.0           0.348202           0.937420\n",
      "      Caribbean           2015              4                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents                No Incidents             0                 0.0                 0.0                 3.0                 0.0                 0.0                 1.0               0.0               0.0               0.0             0.0             0.0             1.0                0.0                0.0                0.0           0.456629           0.889657\n",
      "      Caribbean           2015              5                 1.0       1.0              0.0            0.0          1.0        0.0           0.0            1.0                    Haiti               Caribbean      Mixed or unknown         Dominican Republic Haiti to Dominican Republic             0                 0.0                 0.0                 0.0                 0.0                 0.0                 0.0               0.0               0.0               0.0             0.0             0.0             0.0                0.0                0.0                0.0           0.558647           0.829406\n",
      "      Caribbean           2015              6                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents                No Incidents             0                 0.0                 0.0                 0.0                 1.0                 0.0                 0.0               1.0               0.0               0.0             0.0             0.0             0.0                0.0                0.0                0.0           0.652822           0.757511\n",
      "      Caribbean           2015              7                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents                No Incidents             0                 0.0                 0.0                 0.0                 0.0                 1.0                 0.0               0.0               1.0               0.0             0.0             0.0             0.0                0.0                0.0                0.0           0.737833           0.674983\n",
      "      Caribbean           2015              8                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents                No Incidents             0                 0.0                 0.0                 0.0                 0.0                 0.0                 1.0               0.0               0.0               1.0             0.0             0.0             0.0                0.0                0.0                0.0           0.812487           0.582979\n",
      "      Caribbean           2015              9                 0.0       0.0              0.0            0.0          0.0        0.0           0.0            0.0             No Incidents            No Incidents          No Incidents               No Incidents                No Incidents             0                 0.0                 0.0                 0.0                 0.0                 0.0                 0.0               0.0               0.0               0.0             0.0             0.0             0.0                0.0                0.0                0.0           0.875735           0.482792\n"
     ]
    }
   ],
   "source": [
    "# Create cyclical features for 'incident_week' (replacing incident_month_sin/cos)\n",
    "max_week_for_cycle = df_expanded_weekly['incident_week'].max() # Will be 52 or 53\n",
    "df_expanded_weekly['incident_week_sin'] = np.sin(2 * np.pi * df_expanded_weekly['incident_week'] / max_week_for_cycle)\n",
    "df_expanded_weekly['incident_week_cos'] = np.cos(2 * np.pi * df_expanded_weekly['incident_week'] / max_week_for_cycle)\n",
    "\n",
    "print(\"\\n--- df_expanded_weekly Info after adding pct_change and cyclical week features ---\")\n",
    "print(df_expanded_weekly.info())\n",
    "\n",
    "print(\"\\n--- First 10 rows of df_expanded_weekly with new temporal features ---\")\n",
    "print(df_expanded_weekly.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef0c476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['region_incident', 'incident_year', 'incident_week',\n",
       "       'total_dead_missing', 'num_dead', 'min_est_missing', 'num_survivors',\n",
       "       'num_females', 'num_males', 'num_children', 'num_incidents',\n",
       "       'most_freq_country_origin', 'most_freq_region_origin',\n",
       "       'most_freq_cause_death', 'most_freq_country_incident',\n",
       "       'most_freq_migration_route', 'is_high_risk', 'num_survivors_lag1',\n",
       "       'num_survivors_lag2', 'num_survivors_lag3', 'num_incidents_lag1',\n",
       "       'num_incidents_lag2', 'num_incidents_lag3', 'num_females_lag1',\n",
       "       'num_females_lag2', 'num_females_lag3', 'num_males_lag1',\n",
       "       'num_males_lag2', 'num_males_lag3', 'num_children_lag1',\n",
       "       'num_children_lag2', 'num_children_lag3', 'incident_week_sin',\n",
       "       'incident_week_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_expanded_weekly.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ca9c8bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Features (X) Info ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7049 entries, 0 to 7048\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   region_incident             7049 non-null   object \n",
      " 1   num_survivors               7049 non-null   float64\n",
      " 2   num_incidents               7049 non-null   float64\n",
      " 3   most_freq_country_origin    7049 non-null   object \n",
      " 4   most_freq_region_origin     7049 non-null   object \n",
      " 5   most_freq_cause_death       7049 non-null   object \n",
      " 6   most_freq_country_incident  7049 non-null   object \n",
      " 7   most_freq_migration_route   7049 non-null   object \n",
      " 8   num_survivors_lag1          7049 non-null   float64\n",
      " 9   num_survivors_lag2          7049 non-null   float64\n",
      " 10  num_survivors_lag3          7049 non-null   float64\n",
      " 11  num_incidents_lag1          7049 non-null   float64\n",
      " 12  num_incidents_lag2          7049 non-null   float64\n",
      " 13  num_incidents_lag3          7049 non-null   float64\n",
      " 14  num_females_lag1            7049 non-null   float64\n",
      " 15  num_females_lag2            7049 non-null   float64\n",
      " 16  num_females_lag3            7049 non-null   float64\n",
      " 17  num_males_lag1              7049 non-null   float64\n",
      " 18  num_males_lag2              7049 non-null   float64\n",
      " 19  num_males_lag3              7049 non-null   float64\n",
      " 20  num_children_lag1           7049 non-null   float64\n",
      " 21  num_children_lag2           7049 non-null   float64\n",
      " 22  num_children_lag3           7049 non-null   float64\n",
      " 23  incident_week_sin           7049 non-null   float64\n",
      " 24  incident_week_cos           7049 non-null   float64\n",
      "dtypes: float64(19), object(6)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define features (X) and target (y)\n",
    "# Exclude columns that directly represent the outcome or its components to prevent data leakage\n",
    "# Also exclude incident_year from features as it's used for splitting and direct year can cause overfitting.\n",
    "X = df_expanded_weekly.drop(columns=[\n",
    "    'total_dead_missing',         # Directly used to create 'is_high_risk'\n",
    "    'num_dead',                   # Component of total_dead_missing\n",
    "    'min_est_missing',            # Component of total_dead_missing\n",
    "    'num_females',                # Female casualties - LEAKAGE\n",
    "    'num_males',                  # Male casualties - LEAKAGE\n",
    "    'num_children',               # Child casualties - LEAKAGE\n",
    "    'is_high_risk',               # The target variable itself\n",
    "    'incident_week',              # Cyclical feature, not needed as raw week\n",
    "    'incident_year'               # Used for splitting, typically not a direct feature in this form\n",
    "])\n",
    "y = df_expanded_weekly['is_high_risk']\n",
    "\n",
    "# print X information\n",
    "print(\"\\n--- Features (X) Info ---\")\n",
    "print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1c1a13d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (6023, 25)\n",
      "Shape of X_test: (1026, 25)\n",
      "Shape of y_train: (6023,)\n",
      "Shape of y_test: (1026,)\n",
      "\n",
      "X_train incident_year range: 2014 - 2022\n",
      "X_test incident_year range: 2023 - 2025\n"
     ]
    }
   ],
   "source": [
    "split_year = 2023 # As decided earlier\n",
    "\n",
    "# Perform time-series aware train-test split\n",
    "# Ensure X and y are properly aligned using the index\n",
    "train_indices = df_expanded_weekly['incident_year'] < split_year\n",
    "test_indices = df_expanded_weekly['incident_year'] >= split_year\n",
    "\n",
    "X_train = X[train_indices].copy()\n",
    "X_test = X[test_indices].copy()\n",
    "y_train = y[train_indices].copy()\n",
    "y_test = y[test_indices].copy()\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "# Verify the time ranges\n",
    "print(f\"\\nX_train incident_year range: {df_expanded_weekly[train_indices]['incident_year'].min()} - {df_expanded_weekly[train_indices]['incident_year'].max()}\")\n",
    "print(f\"X_test incident_year range: {df_expanded_weekly[test_indices]['incident_year'].min()} - {df_expanded_weekly[test_indices]['incident_year'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4a36c499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- X_train_encoded head after Target Encoding ---\n",
      " region_incident  num_survivors  num_incidents  most_freq_country_origin  most_freq_region_origin  most_freq_cause_death  most_freq_country_incident  most_freq_migration_route  num_survivors_lag1  num_survivors_lag2  num_survivors_lag3  num_incidents_lag1  num_incidents_lag2  num_incidents_lag3  num_females_lag1  num_females_lag2  num_females_lag3  num_males_lag1  num_males_lag2  num_males_lag3  num_children_lag1  num_children_lag2  num_children_lag3  incident_week_sin  incident_week_cos\n",
      "        0.192429            0.0            0.0                  0.000000                 0.000000               0.000000                    0.000000                   0.052023                 0.0                 0.0                 0.0                 0.0                 0.0                 0.0               0.0               0.0               0.0             0.0             0.0             0.0                0.0                0.0                0.0           0.118273           0.992981\n",
      "        0.192429            3.0            1.0                  0.574074                 0.573643               0.797739                    0.437777                   0.052023                 0.0                 0.0                 0.0                 0.0                 0.0                 0.0               0.0               0.0               0.0             0.0             0.0             0.0                0.0                0.0                0.0           0.118273           0.992981\n",
      "        0.192429            0.0            0.0                  0.000000                 0.000000               0.000000                    0.000000                   0.052023                 3.0                 0.0                 0.0                 1.0                 0.0                 0.0               0.0               0.0               0.0             1.0             0.0             0.0                0.0                0.0                0.0           0.234886           0.972023\n",
      "        0.192429            0.0            0.0                  0.000000                 0.000000               0.000000                    0.000000                   0.052023                 0.0                 3.0                 0.0                 0.0                 1.0                 0.0               0.0               0.0               0.0             0.0             1.0             0.0                0.0                0.0                0.0           0.348202           0.937420\n",
      "        0.192429            0.0            0.0                  0.000000                 0.000000               0.000000                    0.000000                   0.052023                 0.0                 0.0                 3.0                 0.0                 0.0                 1.0               0.0               0.0               0.0             0.0             0.0             1.0                0.0                0.0                0.0           0.456629           0.889657\n",
      "\n",
      "--- X_test_encoded head after Target Encoding ---\n",
      " region_incident  num_survivors  num_incidents  most_freq_country_origin  most_freq_region_origin  most_freq_cause_death  most_freq_country_incident  most_freq_migration_route  num_survivors_lag1  num_survivors_lag2  num_survivors_lag3  num_incidents_lag1  num_incidents_lag2  num_incidents_lag3  num_females_lag1  num_females_lag2  num_females_lag3  num_males_lag1  num_males_lag2  num_males_lag3  num_children_lag1  num_children_lag2  num_children_lag3  incident_week_sin  incident_week_cos\n",
      "        0.192429            0.0            3.0                  0.574074                 0.573643               0.797739                    0.700000                   0.666667                 0.0                34.0                 0.0                 2.0                 2.0                 1.0               5.0               4.0               0.0            29.0            24.0             0.0                0.0                1.0                0.0           0.118273           0.992981\n",
      "        0.192429            0.0            3.0                  0.574074                 0.573643               0.797739                    0.586207                   0.666667                 0.0                 0.0                34.0                 3.0                 2.0                 2.0               3.0               5.0               4.0            29.0            29.0            24.0                1.0                0.0                1.0           0.234886           0.972023\n",
      "        0.192429            0.0            1.0                  0.426667                 0.433333               0.797739                    0.425926                   0.052023                 0.0                 0.0                 0.0                 3.0                 3.0                 2.0               2.0               3.0               5.0            10.0            29.0            29.0                0.0                1.0                0.0           0.348202           0.937420\n",
      "        0.192429           11.0            1.0                  0.574074                 0.573643               0.797739                    0.700000                   0.666667                 0.0                 0.0                 0.0                 1.0                 3.0                 3.0              13.0               2.0               3.0             8.0            10.0            29.0                3.0                0.0                1.0           0.456629           0.889657\n",
      "        0.192429            0.0            0.0                  0.000000                 0.000000               0.000000                    0.000000                   0.052023                11.0                 0.0                 0.0                 1.0                 1.0                 3.0               1.0              13.0               2.0             0.0             8.0            10.0                0.0                3.0                0.0           0.558647           0.829406\n",
      "\n",
      "--- Data types in X_train_encoded after Target Encoding ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6023 entries, 0 to 6994\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   region_incident             6023 non-null   float64\n",
      " 1   num_survivors               6023 non-null   float64\n",
      " 2   num_incidents               6023 non-null   float64\n",
      " 3   most_freq_country_origin    6023 non-null   float64\n",
      " 4   most_freq_region_origin     6023 non-null   float64\n",
      " 5   most_freq_cause_death       6023 non-null   float64\n",
      " 6   most_freq_country_incident  6023 non-null   float64\n",
      " 7   most_freq_migration_route   6023 non-null   float64\n",
      " 8   num_survivors_lag1          6023 non-null   float64\n",
      " 9   num_survivors_lag2          6023 non-null   float64\n",
      " 10  num_survivors_lag3          6023 non-null   float64\n",
      " 11  num_incidents_lag1          6023 non-null   float64\n",
      " 12  num_incidents_lag2          6023 non-null   float64\n",
      " 13  num_incidents_lag3          6023 non-null   float64\n",
      " 14  num_females_lag1            6023 non-null   float64\n",
      " 15  num_females_lag2            6023 non-null   float64\n",
      " 16  num_females_lag3            6023 non-null   float64\n",
      " 17  num_males_lag1              6023 non-null   float64\n",
      " 18  num_males_lag2              6023 non-null   float64\n",
      " 19  num_males_lag3              6023 non-null   float64\n",
      " 20  num_children_lag1           6023 non-null   float64\n",
      " 21  num_children_lag2           6023 non-null   float64\n",
      " 22  num_children_lag3           6023 non-null   float64\n",
      " 23  incident_week_sin           6023 non-null   float64\n",
      " 24  incident_week_cos           6023 non-null   float64\n",
      "dtypes: float64(25)\n",
      "memory usage: 1.2 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns that need encoding (these are the 'most_freq' and region_incident)\n",
    "# Ensure these columns exist in X_train/X_test\n",
    "categorical_cols_for_encoding = [\n",
    "    'region_incident',\n",
    "    'most_freq_country_origin',\n",
    "    'most_freq_region_origin',\n",
    "    'most_freq_cause_death',\n",
    "    'most_freq_country_incident',\n",
    "    'most_freq_migration_route'\n",
    "]\n",
    "\n",
    "# Create a copy to avoid SettingWithCopyWarning during encoding\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "# Initialize TargetEncoder\n",
    "# 'smoothing' and 'min_samples_leaf' help prevent overfitting by balancing\n",
    "# the categorical average with the global average, especially important for rare categories.\n",
    "encoder = TargetEncoder(cols=categorical_cols_for_encoding, smoothing=1.0, min_samples_leaf=1)\n",
    "\n",
    "# Fit the encoder on the training data (X_train_encoded and y_train)\n",
    "encoder.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Transform both training and test data\n",
    "X_train_encoded = encoder.transform(X_train_encoded)\n",
    "X_test_encoded = encoder.transform(X_test_encoded)\n",
    "\n",
    "print(\"\\n--- X_train_encoded head after Target Encoding ---\")\n",
    "print(X_train_encoded.head().to_string(index=False))\n",
    "\n",
    "print(\"\\n--- X_test_encoded head after Target Encoding ---\")\n",
    "print(X_test_encoded.head().to_string(index=False))\n",
    "\n",
    "print(\"\\n--- Data types in X_train_encoded after Target Encoding ---\")\n",
    "print(X_train_encoded.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4f580626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Class distribution before SMOTE ---\n",
      "is_high_risk\n",
      "0    4405\n",
      "1    1618\n",
      "\n",
      "--- Class distribution after SMOTE ---\n",
      "is_high_risk\n",
      "0    4405\n",
      "1    4405\n",
      "\n",
      "Shape of X_train_resampled: (8810, 25)\n",
      "Shape of y_train_resampled: (8810,)\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to the training data only\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "print(\"\\n--- Class distribution before SMOTE ---\")\n",
    "print(y_train.value_counts().to_string())\n",
    "print(\"\\n--- Class distribution after SMOTE ---\")\n",
    "print(y_train_resampled.value_counts().to_string())\n",
    "\n",
    "print(f\"\\nShape of X_train_resampled: {X_train_resampled.shape}\")\n",
    "print(f\"Shape of y_train_resampled: {y_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "07fc579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Baseline Random Forest Classifier Performance ---\n",
      "Accuracy: 0.9337\n",
      "Precision: 0.9216\n",
      "Recall: 0.8916\n",
      "F1 Score: 0.9063\n",
      "\n",
      "--- Feature Importances from Random Forest ---\n",
      "                       feature  importance\n",
      "2                num_incidents   22.542262\n",
      "6   most_freq_country_incident   18.476182\n",
      "3     most_freq_country_origin   18.266835\n",
      "5        most_freq_cause_death    9.179675\n",
      "4      most_freq_region_origin    7.300511\n",
      "7    most_freq_migration_route    5.161491\n",
      "12          num_incidents_lag2    3.259012\n",
      "0              region_incident    3.002721\n",
      "19              num_males_lag3    2.563140\n",
      "1                num_survivors    1.843414\n",
      "13          num_incidents_lag3    1.522611\n",
      "23           incident_week_sin    1.200492\n",
      "18              num_males_lag2    1.130286\n",
      "24           incident_week_cos    1.097923\n",
      "11          num_incidents_lag1    0.866264\n",
      "17              num_males_lag1    0.559698\n",
      "8           num_survivors_lag1    0.350879\n",
      "9           num_survivors_lag2    0.304411\n",
      "10          num_survivors_lag3    0.266493\n",
      "14            num_females_lag1    0.227711\n",
      "15            num_females_lag2    0.213221\n",
      "16            num_females_lag3    0.205850\n",
      "20           num_children_lag1    0.188015\n",
      "21           num_children_lag2    0.136088\n",
      "22           num_children_lag3    0.134814\n"
     ]
    }
   ],
   "source": [
    "rf_baseline = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "rf_baseline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred = rf_baseline.predict(X_test_encoded)\n",
    "\n",
    "print(\"\\n--- Baseline Random Forest Classifier Performance ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_resampled.columns,\n",
    "    'importance': rf_baseline.feature_importances_ * 100\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importances from Random Forest ---\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cc5513a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 15 features:\n",
      "                   feature  importance  cumulative_importance\n",
      "             num_incidents   22.542262              22.542262\n",
      "most_freq_country_incident   18.476182              41.018445\n",
      "  most_freq_country_origin   18.266835              59.285280\n",
      "     most_freq_cause_death    9.179675              68.464955\n",
      "   most_freq_region_origin    7.300511              75.765466\n",
      " most_freq_migration_route    5.161491              80.926957\n",
      "        num_incidents_lag2    3.259012              84.185969\n",
      "           region_incident    3.002721              87.188690\n",
      "            num_males_lag3    2.563140              89.751830\n",
      "             num_survivors    1.843414              91.595243\n",
      "        num_incidents_lag3    1.522611              93.117854\n",
      "         incident_week_sin    1.200492              94.318346\n",
      "            num_males_lag2    1.130286              95.448633\n",
      "         incident_week_cos    1.097923              96.546556\n",
      "        num_incidents_lag1    0.866264              97.412821\n",
      "\n",
      "Cumulative importance threshold for top 15 features: 97.41%\n"
     ]
    }
   ],
   "source": [
    "top_k = 15\n",
    "important_features = feature_importance.head(top_k)['feature'].tolist()\n",
    "\n",
    "# find cumulative importance threshold from feature_importance\n",
    "cumulative_threshold = feature_importance['cumulative_importance'].iloc[top_k - 1]\n",
    "\n",
    "print(f\"Selected {len(important_features)} features:\")\n",
    "print(feature_importance.head(top_k).to_string(index=False))\n",
    "\n",
    "print(f\"\\nCumulative importance threshold for top {top_k} features: {cumulative_threshold:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "878f498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X_train_reduced: (8810, 15)\n",
      "Shape of X_test_reduced: (1026, 15)\n"
     ]
    }
   ],
   "source": [
    "X_train_reduced = X_train_resampled[important_features]\n",
    "X_test_reduced = X_test_encoded[important_features]\n",
    "\n",
    "print(f\"\\nShape of X_train_reduced: {X_train_reduced.shape}\")\n",
    "print(f\"Shape of X_test_reduced: {X_test_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # enhance the param_grid into much more comprehensive search space\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'max_depth': [10, 15, 20, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# tscv = TimeSeriesSplit(n_splits=5)\n",
    "# rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=rf_classifier,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='f1',\n",
    "#     cv=tscv,\n",
    "#     n_jobs=1,\n",
    "#     verbose=1,\n",
    "#     return_train_score=True,\n",
    "#     refit=True\n",
    "# )\n",
    "\n",
    "# grid_search.fit(X_train_reduced, y_train_resampled)\n",
    "\n",
    "# print(\"\\n--- Best Parameters from Grid Search ---\")\n",
    "# print(grid_search.best_params_)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# timestamp = int(datetime.datetime.now().timestamp())\n",
    "\n",
    "# with open(f'models/best_model_{timestamp}.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40611b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 400),          # Random integers between 50-399\n",
    "    'max_depth': [10, 15, 20, 25, 30, None],   # Discrete choices including None\n",
    "    'min_samples_split': randint(2, 15),       # Random integers between 2-14\n",
    "    'min_samples_leaf': randint(1, 8),         # Random integers between 1-7\n",
    "    'max_features': ['sqrt', 'log2', None],    # Additional parameter for variety\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV with more parameter combinations but fewer total fits\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring='f1',\n",
    "    cv=tscv,\n",
    "    n_jobs=1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    return_train_score=True,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# Fit the random search\n",
    "random_search.fit(X_train_reduced, y_train_resampled)\n",
    "\n",
    "print(\"\\n--- Best Parameters from Random Search ---\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"\\n--- Best CV Score: {random_search.best_score_:.4f} ---\")\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Save the model\n",
    "timestamp = int(datetime.datetime.now().timestamp())\n",
    "with open(f'models/best_model_{timestamp}.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46314023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from saved model with highest timestamp for each type of migrants\n",
    "\n",
    "def load_best_model():\n",
    "    model_files = glob.glob(f'models/best_model_*.pkl')\n",
    "\n",
    "    if not model_files:\n",
    "        raise FileNotFoundError(f\"No saved models found.\")\n",
    "\n",
    "    # Sort the files by timestamp (filename) and get the latest one\n",
    "    latest_model_file = max(model_files, key=os.path.getctime)\n",
    "\n",
    "    with open(latest_model_file, 'rb') as f:\n",
    "        print(f\"Loading model from: {latest_model_file}\")\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    return model\n",
    "\n",
    "best_model = load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "y_pred = best_model.predict(X_test_reduced)\n",
    "\n",
    "print(\"\\n--- Best Model Performance on Test Set ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fb2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['Low Risk', 'High Risk'],\n",
    "    yticklabels=['Low Risk', 'High Risk']\n",
    ")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(top_k))\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.xlabel('Importance (%)')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Top 10 Features and Their Importance ---\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-3.12-base-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
